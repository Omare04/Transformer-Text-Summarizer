

---

# Transformer Summarizer

This assignment is part of Course 4, Week 2, in the NLP specialization, where I implemented a Transformer model for text summarization using an encoder-decoder architecture.


### Key Topics Covered

1. **Dataset Importing**: Load and prepare a dataset for summarization tasks.
2. **Data Preprocessing**: Tokenize, encode, and prepare data for input into the Transformer model.
3. **Positional Encoding**: Implement positional encodings to allow the model to learn the order of tokens.
4. **Masking**: Create padding and look-ahead masks essential for model training.
5. **Self-attention Mechanism**: Implement the core attention mechanism, specifically scaled dot-product attention.
6. **Encoder Architecture**:
   - **Encoder Layer**: Build a single layer of the encoder.
   - **Full Encoder**: Combine layers to form the complete encoder component.
7. **Decoder Architecture**:
   - **Decoder Layer**: Develop the core decoding layer from scratch.
   - **Full Decoder**: Assemble layers to form the complete decoder.
8. **Training**: Train the model using the summarization dataset, evaluating its performance and fine-tuning as needed.

## File Structure

- **`notebook.ipynb`**: Main Jupyter notebook containing instructions, code cells, and exercises.
- **`README.md`**: (This file) Overview of the project, guiding you through each module.
- **`data/`**: Directory to store dataset files.
- **`utils.py`**: Optional helper functions for model training and evaluation (if included in your project).


## Acknowledgments

This assignment is part of the NLP specialization by Deeplearning.ai. 
--- 
